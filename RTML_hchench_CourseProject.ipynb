{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **LPRNet Model**"
      ],
      "metadata": {
        "id": "LdZJjBs0Xvnn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEJV9dA_TqFV",
        "outputId": "8b997e3f-32b8-4948-d98e-2b1dab27bd9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LPRNet_Pytorch'...\n",
            "remote: Enumerating objects: 1071, done.\u001b[K\n",
            "remote: Counting objects: 100% (34/34), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 1071 (delta 25), reused 22 (delta 22), pack-reused 1037 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1071/1071), 20.04 MiB | 9.92 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "/content/LPRNet_Pytorch\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sirius-ai/LPRNet_Pytorch.git\n",
        "%cd LPRNet_Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7YfqbtOG2A9",
        "outputId": "243b7905-86c0-4d7f-c45f-fa7104110036"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data  LICENSE  model  README.md  test_LPRNet.py  train_LPRNet.py  weights\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overwriting cv2.destroyAllWindows()"
      ],
      "metadata": {
        "id": "x8Esw9pdW4s6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"test_LPRNet.py\"\n",
        "with open(file_path, \"r\") as file:\n",
        "    lines = file.readlines()\n",
        "\n",
        "with open(file_path, \"w\") as file:\n",
        "    inside_finally = False\n",
        "    for line in lines:\n",
        "        if line.strip() == 'finally:':\n",
        "            inside_finally = True\n",
        "            file.write(line)  # write the finally line\n",
        "        elif inside_finally and line.strip() == '':\n",
        "            inside_finally = False  # end of finally block\n",
        "            file.write(line)\n",
        "        elif inside_finally:\n",
        "            if \"cv2.destroyAllWindows()\" in line:\n",
        "                # Replace cv2.destroyAllWindows() with pass, ensuring proper indentation\n",
        "                file.write('         pass\\n')  # Ensuring 'pass' is indented correctly\n",
        "            else:\n",
        "                # Indent other lines inside finally block\n",
        "                file.write('    ' + line)\n",
        "        else:\n",
        "            file.write(line)"
      ],
      "metadata": {
        "id": "pzKNpusBW8oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LPRNet Model Accuracy and Inference Speed**"
      ],
      "metadata": {
        "id": "2KZ_aZNiOLjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OYohI_9lYFY",
        "outputId": "bf6da96f-533c-4fb6-ffcc-1afc0e46c426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful to build network!\n",
            "/content/LPRNet_Pytorch/./test_LPRNet.py:65: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  lprnet.load_state_dict(torch.load(args.pretrained_model))\n",
            "load pretrained model successful!\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "[Info] Test Accuracy: 0.899 [899:61:40:1000]\n",
            "[Info] Test Speed: 0.0015159180164337157s 1/1000]\n"
          ]
        }
      ],
      "source": [
        "!python ./test_LPRNet.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installations\n",
        "!apt-get install -y llvm-14-dev\n",
        "!pip install apache-tvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0p7UQi2XAYC",
        "outputId": "540e60f6-ca55-46fc-8232-d5eefed53b77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  binfmt-support libffi-dev libpfm4 libz3-4 libz3-dev llvm-14 llvm-14-runtime llvm-14-tools\n",
            "  python3-pygments python3-yaml\n",
            "Suggested packages:\n",
            "  llvm-14-doc python-pygments-doc ttf-bitstream-vera\n",
            "The following NEW packages will be installed:\n",
            "  binfmt-support libffi-dev libpfm4 libz3-4 libz3-dev llvm-14 llvm-14-dev llvm-14-runtime\n",
            "  llvm-14-tools python3-pygments python3-yaml\n",
            "0 upgraded, 11 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 58.6 MB of archives.\n",
            "After this operation, 354 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 python3-yaml amd64 5.4.1-1ubuntu1 [129 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 binfmt-support amd64 2.2.1-2 [55.8 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14-runtime amd64 1:14.0.0-1ubuntu1.1 [484 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpfm4 amd64 4.11.1+git32-gd0b85fb-1ubuntu0.1 [345 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14 amd64 1:14.0.0-1ubuntu1.1 [12.7 MB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libffi-dev amd64 3.4.2-4 [63.7 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-pygments all 2.11.2+dfsg-2ubuntu0.1 [750 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14-tools amd64 1:14.0.0-1ubuntu1.1 [404 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libz3-4 amd64 4.8.12-1 [5,766 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libz3-dev amd64 4.8.12-1 [72.2 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 llvm-14-dev amd64 1:14.0.0-1ubuntu1.1 [37.8 MB]\n",
            "Fetched 58.6 MB in 6s (10.4 MB/s)\n",
            "Selecting previously unselected package python3-yaml.\n",
            "(Reading database ... 123632 files and directories currently installed.)\n",
            "Preparing to unpack .../00-python3-yaml_5.4.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking python3-yaml (5.4.1-1ubuntu1) ...\n",
            "Selecting previously unselected package binfmt-support.\n",
            "Preparing to unpack .../01-binfmt-support_2.2.1-2_amd64.deb ...\n",
            "Unpacking binfmt-support (2.2.1-2) ...\n",
            "Selecting previously unselected package llvm-14-runtime.\n",
            "Preparing to unpack .../02-llvm-14-runtime_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14-runtime (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libpfm4:amd64.\n",
            "Preparing to unpack .../03-libpfm4_4.11.1+git32-gd0b85fb-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libpfm4:amd64 (4.11.1+git32-gd0b85fb-1ubuntu0.1) ...\n",
            "Selecting previously unselected package llvm-14.\n",
            "Preparing to unpack .../04-llvm-14_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14 (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libffi-dev:amd64.\n",
            "Preparing to unpack .../05-libffi-dev_3.4.2-4_amd64.deb ...\n",
            "Unpacking libffi-dev:amd64 (3.4.2-4) ...\n",
            "Selecting previously unselected package python3-pygments.\n",
            "Preparing to unpack .../06-python3-pygments_2.11.2+dfsg-2ubuntu0.1_all.deb ...\n",
            "Unpacking python3-pygments (2.11.2+dfsg-2ubuntu0.1) ...\n",
            "Selecting previously unselected package llvm-14-tools.\n",
            "Preparing to unpack .../07-llvm-14-tools_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14-tools (1:14.0.0-1ubuntu1.1) ...\n",
            "Selecting previously unselected package libz3-4:amd64.\n",
            "Preparing to unpack .../08-libz3-4_4.8.12-1_amd64.deb ...\n",
            "Unpacking libz3-4:amd64 (4.8.12-1) ...\n",
            "Selecting previously unselected package libz3-dev:amd64.\n",
            "Preparing to unpack .../09-libz3-dev_4.8.12-1_amd64.deb ...\n",
            "Unpacking libz3-dev:amd64 (4.8.12-1) ...\n",
            "Selecting previously unselected package llvm-14-dev.\n",
            "Preparing to unpack .../10-llvm-14-dev_1%3a14.0.0-1ubuntu1.1_amd64.deb ...\n",
            "Unpacking llvm-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up python3-yaml (5.4.1-1ubuntu1) ...\n",
            "Setting up libffi-dev:amd64 (3.4.2-4) ...\n",
            "Setting up python3-pygments (2.11.2+dfsg-2ubuntu0.1) ...\n",
            "Setting up libz3-4:amd64 (4.8.12-1) ...\n",
            "Setting up libpfm4:amd64 (4.11.1+git32-gd0b85fb-1ubuntu0.1) ...\n",
            "Setting up llvm-14-runtime (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up binfmt-support (2.2.1-2) ...\n",
            "invoke-rc.d: could not determine current runlevel\n",
            "invoke-rc.d: policy-rc.d denied execution of restart.\n",
            "Created symlink /etc/systemd/system/multi-user.target.wants/binfmt-support.service → /lib/systemd/system/binfmt-support.service.\n",
            "Setting up llvm-14 (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up llvm-14-tools (1:14.0.0-1ubuntu1.1) ...\n",
            "Setting up libz3-dev:amd64 (4.8.12-1) ...\n",
            "Setting up llvm-14-dev (1:14.0.0-1ubuntu1.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "Collecting apache-tvm\n",
            "  Downloading apache_tvm-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (24.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (3.1.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.26.4)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (1.13.1)\n",
            "Collecting synr==0.6.0 (from apache-tvm)\n",
            "  Downloading synr-0.6.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (6.3.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from apache-tvm) (4.12.2)\n",
            "Downloading apache_tvm-0.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading synr-0.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: synr, apache-tvm\n",
            "Successfully installed apache-tvm-0.11.1 synr-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from model.LPRNet import build_lprnet\n",
        "import os\n",
        "import torch.nn.utils.prune as prune\n",
        "import copy\n",
        "from data.load_data import CHARS, CHARS_DICT, LPRDataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import *\n",
        "import argparse\n",
        "import tvm\n",
        "from tvm import te,relay\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "xf20S6I9XPzl"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision onnx onnxruntime tvm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gczJBRNsYHR3",
        "outputId": "2af86088-c5b4-40aa-9df3-0a1c111f93ca"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting tvm\n",
            "  Downloading tvm-1.0.0.tar.gz (5.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx) (4.25.5)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.3.25)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime) (24.2)\n",
            "Collecting appdirs (from tvm)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting docopt (from tvm)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting inform (from tvm)\n",
            "  Downloading inform-1.32-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting quantiphy (from tvm)\n",
            "  Downloading quantiphy-2.20-py3-none-any.whl.metadata (7.7 kB)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting arrow (from inform->tvm)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from inform->tvm) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from arrow->inform->tvm) (2.8.2)\n",
            "Collecting types-python-dateutil>=2.8.10 (from arrow->inform->tvm)\n",
            "  Downloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl.metadata (2.1 kB)\n",
            "Downloading onnx-1.17.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m94.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading inform-1.32-py3-none-any.whl (39 kB)\n",
            "Downloading quantiphy-2.20-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_python_dateutil-2.9.0.20241206-py3-none-any.whl (14 kB)\n",
            "Building wheels for collected packages: tvm, docopt\n",
            "  Building wheel for tvm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tvm: filename=tvm-1.0.0-py3-none-any.whl size=5084 sha256=dcdc64b5d2226eaf419d3a9f23748d653f8a0554f80e3a546b641f46692048d7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/31/8c/025d5271ffd5a09fa26522edb4cdbb3d532c2f254a3bbb7c40\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=cdd2c22be77fa27889e40f60ac86e5fd4e2979d6a07c3a324d4ac277b139b86d\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "Successfully built tvm docopt\n",
            "Installing collected packages: docopt, appdirs, types-python-dateutil, quantiphy, onnx, humanfriendly, coloredlogs, arrow, onnxruntime, inform, tvm\n",
            "Successfully installed appdirs-1.4.4 arrow-1.3.0 coloredlogs-15.0.1 docopt-0.6.2 humanfriendly-10.0 inform-1.32 onnx-1.17.0 onnxruntime-1.20.1 quantiphy-2.20 tvm-1.0.0 types-python-dateutil-2.9.0.20241206\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_size_onnx(model, input_tensor=None):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # If no input tensor is provided, use a default one based on the model's input shape\n",
        "    if input_tensor is None:\n",
        "        input_tensor = torch.randn(1, 3, 24, 94)\n",
        "\n",
        "    # Export the model to ONNX format\n",
        "    onnx_file = \"./weights/model.onnx\"\n",
        "    torch.onnx.export(model, input_tensor, onnx_file, verbose=False, opset_version=12)\n",
        "\n",
        "    # Get the size of the ONNX model file in bytes\n",
        "    model_size_bytes = os.path.getsize(onnx_file)\n",
        "\n",
        "    # Convert bytes to MB (1 MB = 1024 * 1024 bytes)\n",
        "    model_size_MB = model_size_bytes / (1024 ** 2)\n",
        "\n",
        "\n",
        "    return model_size_MB\n"
      ],
      "metadata": {
        "id": "7PnCf0QcYOX2"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "lprnet_model = build_lprnet()\n",
        "lprnet_model.eval()\n",
        "\n",
        "# Prepare dummy input\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dummy_input = torch.randn(1, 3, 24, 94).to(device)  # Dummy input for ONNX export\n",
        "lprnet_model.to(device)\n",
        "\n",
        "# ONNX export path\n",
        "onnx_model_path = './weights/lprnet_model.onnx'\n",
        "\n",
        "# Export the model\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        lprnet_model,\n",
        "        dummy_input,\n",
        "        onnx_model_path,\n",
        "        export_params=True,  # Store the trained parameter weights\n",
        "        opset_version=11,    # Use an ONNX version compatible with your use case\n",
        "        do_constant_folding=True,  # Optimize constant folding\n",
        "        input_names=['input'],  # Name of the input tensor\n",
        "        output_names=['output'],  # Name of the output tensor\n",
        "        dynamic_axes={\n",
        "            'input': {0: 'batch_size'},  # Allow dynamic batch sizes\n",
        "            'output': {0: 'batch_size'}\n",
        "        },\n",
        "        verbose=True\n",
        "    )\n",
        "    print(f'Model exported successfully to: {onnx_model_path}')\n",
        "except Exception as e:\n",
        "    print(f\"Error exporting the model: {e}\")"
      ],
      "metadata": {
        "id": "CofRuxZ-XJjp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5693f5c-92ca-4adb-cf5d-6f05a283ff0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model exported successfully to: ./weights/lprnet_model.onnx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LPRNet Model Size**"
      ],
      "metadata": {
        "id": "1FfVXf2iNgAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the file size of the ONNX model\n",
        "onnx_size = os.path.getsize(onnx_model_path) / (1024 * 1024)  # Size in MB\n",
        "print(f\"LPRNet model size: {onnx_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijK1Mj-6ZEnb",
        "outputId": "37db36f5-4f98-408b-8ca5-04756061b77e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LPRNet model size: 1.89 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PS6acuLP0pKZ",
        "outputId": "365cd94b-dd43-4b21-c6b1-c883eb0cebab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries if not already installed\n",
        "!pip install torch torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MODEL OPTIMISATIONS**"
      ],
      "metadata": {
        "id": "wqH9G3kwZg4l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iv49DvnT_NL7"
      },
      "source": [
        "##**1.   PRUNING**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "idARYnin3FjJ"
      },
      "outputs": [],
      "source": [
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class LPRNet_pruned(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet_pruned, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            small_basic_block(ch_in=256, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1),\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]:  # Feature layers to keep\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            global_context.append(f)\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TBliyV933fcA"
      },
      "outputs": [],
      "source": [
        "def apply_pruning(model, amount=0.05):\n",
        "    \"\"\"\n",
        "    Applies pruning to the model's layers to improve performance.\n",
        "    \"\"\"\n",
        "    model_pruned = copy.deepcopy(model)  # Create a deep copy of the original model\n",
        "\n",
        "    # Apply L1 pruning to Conv2D and Linear layers\n",
        "    for name, module in model_pruned.named_modules():\n",
        "        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "            prune.remove(module, 'weight')\n",
        "            print(f\"Pruned {name} layer\")\n",
        "\n",
        "    print(\"Pruning applied successfully!\")\n",
        "    return model_pruned\n",
        "\n",
        "def build_pruned_model(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5, pruning_amount=0.05):\n",
        "    \"\"\"\n",
        "    Builds a pruned version of the LPRNet model by first constructing the model,\n",
        "    then applying pruning to it.\n",
        "    \"\"\"\n",
        "    model = LPRNet_pruned(lpr_max_len, phase, class_num, dropout_rate)\n",
        "\n",
        "    if phase == \"train\":\n",
        "        return model.train()\n",
        "    else:\n",
        "        return model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruned LPRNet Model Size"
      ],
      "metadata": {
        "id": "IGKG-d6r4fCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet_model = build_lprnet(lpr_max_len=8,\n",
        "        phase=False,\n",
        "        class_num=60,\n",
        "        dropout_rate=0.5)\n",
        "pruned_model = apply_pruning(lprnet_model)\n",
        "pruned_model.eval()\n",
        "\n",
        "onnx_size = get_model_size_onnx(pruned_model)\n",
        "print(f\"LPRNet model size: {onnx_size:.4f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rcphCaxES-3A",
        "outputId": "b580331b-925a-4486-8ba1-ba2715846bac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pruned backbone.0 layer\n",
            "Pruned backbone.4.block.0 layer\n",
            "Pruned backbone.4.block.2 layer\n",
            "Pruned backbone.4.block.4 layer\n",
            "Pruned backbone.4.block.6 layer\n",
            "Pruned backbone.8.block.0 layer\n",
            "Pruned backbone.8.block.2 layer\n",
            "Pruned backbone.8.block.4 layer\n",
            "Pruned backbone.8.block.6 layer\n",
            "Pruned backbone.11.block.0 layer\n",
            "Pruned backbone.11.block.2 layer\n",
            "Pruned backbone.11.block.4 layer\n",
            "Pruned backbone.11.block.6 layer\n",
            "Pruned backbone.16 layer\n",
            "Pruned backbone.20 layer\n",
            "Pruned container.0 layer\n",
            "Pruning applied successfully!\n",
            "LPRNet model size: 1.5871 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXa7Nwbu4XNH"
      },
      "outputs": [],
      "source": [
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser(description='parameters to test pruned net')\n",
        "    parser.add_argument('--img_size', default=[94, 24], help='the image size')\n",
        "    parser.add_argument('--test_img_dirs', default=\"./data/test\", help='the test images path')\n",
        "    parser.add_argument('--dropout_rate', default=0, help='dropout rate.')\n",
        "    parser.add_argument('--lpr_max_len', default=8, help='license plate number max length.')\n",
        "    parser.add_argument('--test_batch_size', default=100, help='testing batch size.')\n",
        "    parser.add_argument('--phase_train', default=False, type=bool, help='train or test phase flag.')\n",
        "    parser.add_argument('--num_workers', default=8, type=int, help='Number of workers used in dataloading')\n",
        "    parser.add_argument('--cuda', default=True, type=bool, help='Use cuda to test model')\n",
        "    parser.add_argument('--show', default=False, type=bool, help='show test image and its predict result or not.')\n",
        "    parser.add_argument('--pretrained_model', default='./weights/Final_LPRNet_model.pth', help='pretrained base model')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMOrXCHD5YlT"
      },
      "outputs": [],
      "source": [
        "def Greedy_Decode_Eval(Net, datasets, args):\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    print(\"[Info] epoch_size: %d\" % epoch_size)\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "    for i in range(epoch_size):\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start + length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "\n",
        "        if args.cuda:\n",
        "            images = Variable(images.cuda())\n",
        "        else:\n",
        "            images = Variable(images)\n",
        "\n",
        "        # forward\n",
        "        prebs = Net(images)\n",
        "        prebs = prebs.cpu().detach().numpy()\n",
        "\n",
        "        # Greedy decode\n",
        "        preb_labels = []\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = []\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = []\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label:\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp + Tn_1 + Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wh-oaOOz5PAg"
      },
      "outputs": [],
      "source": [
        "# Replace cv2.destroyAllWindows() with a Colab-friendly visualization method\n",
        "def close_visualizations():\n",
        "    plt.close('all')  # Closes all open matplotlib figures\n",
        "\n",
        "def test_pruned():\n",
        "    args = get_parser()\n",
        "\n",
        "    # Build LPRNet_pruned\n",
        "    lprnet_pruned = build_pruned_model(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "    lprnet_pruned.to(device)\n",
        "    print(\"Successfully built pruned network!\")\n",
        "\n",
        "    # Load the pretrained model and filter pruned weights\n",
        "    if args.pretrained_model:\n",
        "        checkpoint = torch.load(args.pretrained_model, map_location=device, weights_only=True)\n",
        "        filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in lprnet_pruned.state_dict()}\n",
        "        lprnet_pruned.load_state_dict(filtered_checkpoint, strict=False)\n",
        "        print(\"Successfully loaded pruned pretrained model!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't find pretrained model, please check!\")\n",
        "        return False\n",
        "\n",
        "    # Prepare the test dataset\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "\n",
        "    try:\n",
        "        Greedy_Decode_Eval(lprnet_pruned, test_dataset, args)\n",
        "    finally:\n",
        "        close_visualizations()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pruned LPRNet Model Accuracy and Inference Speed"
      ],
      "metadata": {
        "id": "vjSolyTj4nZP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyfB_wv298AO",
        "outputId": "5a87b3ab-6698-4973-9899-b634676e6747"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully built network!\n",
            "Pruned backbone.0 layer\n",
            "Pruned backbone.4.block.0 layer\n",
            "Pruned backbone.4.block.2 layer\n",
            "Pruned backbone.4.block.4 layer\n",
            "Pruned backbone.4.block.6 layer\n",
            "Pruned backbone.8.block.0 layer\n",
            "Pruned backbone.8.block.2 layer\n",
            "Pruned backbone.8.block.4 layer\n",
            "Pruned backbone.8.block.6 layer\n",
            "Pruned backbone.11.block.0 layer\n",
            "Pruned backbone.11.block.2 layer\n",
            "Pruned backbone.11.block.4 layer\n",
            "Pruned backbone.11.block.6 layer\n",
            "Pruned backbone.16 layer\n",
            "Pruned backbone.20 layer\n",
            "Pruned container.0 layer\n",
            "Pruning applied successfully!\n",
            "Successfully loaded pruned pretrained model!\n",
            "[Info] epoch_size: 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-49cc2eb8cdb9>:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.pretrained_model, map_location=device)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.896 [896:63:41:1000]\n",
            "[Info] Test Speed: 0.0006168961524963378s 1/1000]\n"
          ]
        }
      ],
      "source": [
        "def test_pruned():\n",
        "    # Manually set the arguments as needed\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.img_size = [94, 24]\n",
        "            self.test_img_dirs = './data/test'\n",
        "            self.dropout_rate = 0.5\n",
        "            self.lpr_max_len = 8\n",
        "            self.test_batch_size = 100\n",
        "            self.phase_train = False\n",
        "            self.num_workers = 8\n",
        "            self.cuda = True\n",
        "            self.show = False\n",
        "            self.pretrained_model = './weights/Final_LPRNet_model.pth'\n",
        "\n",
        "    args = Args()\n",
        "    lprnet= build_lprnet(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "    lprnet.to(device)\n",
        "    print(\"Successfully built network!\")\n",
        "    if args.pretrained_model:\n",
        "        checkpoint = torch.load(args.pretrained_model, map_location=device)\n",
        "        filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in lprnet.state_dict()}\n",
        "        lprnet.load_state_dict(filtered_checkpoint, strict=False)\n",
        "        pruned_model=apply_pruning(lprnet, amount=0.20)\n",
        "        print(\"Successfully loaded pruned pretrained model!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't find pretrained model, please check!\")\n",
        "        return False\n",
        "\n",
        "    # Prepare the test dataset\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "\n",
        "    try:\n",
        "        Greedy_Decode_Eval(pruned_model, test_dataset, args)\n",
        "    finally:\n",
        "        close_visualizations()\n",
        "\n",
        "test_pruned()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiFSLbGtGlAf"
      },
      "source": [
        "##**2. QUANTIZATION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tX75yT1Gspm"
      },
      "outputs": [],
      "source": [
        "class LPRNet_quantized(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet_quantized, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            small_basic_block(ch_in=256, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1),\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]:\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            global_context.append(f)\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHLb9KiMGxww"
      },
      "outputs": [],
      "source": [
        "def fuse_conv_bn(conv, bn):\n",
        "    \"\"\"\n",
        "    Correctly fuse Conv2D and BatchNorm2D layers.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():\n",
        "        conv_weight = conv.weight.clone()\n",
        "        conv_bias = conv.bias if conv.bias is not None else torch.zeros(conv.out_channels, device=conv.weight.device)\n",
        "\n",
        "        # BatchNorm parameters\n",
        "        bn_weight = bn.weight\n",
        "        bn_bias = bn.bias\n",
        "        bn_mean = bn.running_mean\n",
        "        bn_var = bn.running_var\n",
        "        bn_eps = bn.eps\n",
        "\n",
        "        scale = bn_weight / torch.sqrt(bn_var + bn_eps)\n",
        "        bias = bn_bias - bn_mean * scale\n",
        "\n",
        "        # Fuse Conv weights and biases\n",
        "        fused_weight = conv_weight * scale.view(-1, 1, 1, 1)\n",
        "        fused_bias = conv_bias * scale + bias\n",
        "\n",
        "        # Update Conv2D layer\n",
        "        conv.weight.copy_(fused_weight)\n",
        "        conv.bias = nn.Parameter(fused_bias)\n",
        "    return conv\n",
        "\n",
        "def fuse_lprnet_model(model):\n",
        "    \"\"\"\n",
        "    Fuse all Conv2D and BatchNorm2D layers in the LPRNet_quantized model.\n",
        "    \"\"\"\n",
        "    for name, module in model.backbone.named_children():\n",
        "        if isinstance(module, nn.Conv2d):\n",
        "            # Check for a BatchNorm2D layer following the Conv2D\n",
        "            next_name = str(int(name) + 1)\n",
        "            if next_name in model.backbone._modules:\n",
        "                next_module = model.backbone._modules[next_name]\n",
        "                if isinstance(next_module, nn.BatchNorm2d):\n",
        "                    # Fuse Conv2D and BatchNorm2D\n",
        "                    fused_conv = fuse_conv_bn(module, next_module)\n",
        "                    # Replace layers in the model\n",
        "                    model.backbone._modules[name] = fused_conv\n",
        "                    model.backbone._modules[next_name] = nn.Identity()\n",
        "    return model\n",
        "\n",
        "def apply_quantization(model):\n",
        "    \"\"\"\n",
        "    Applies dynamic quantization to the model to improve inference performance.\n",
        "    \"\"\"\n",
        "    model_quantized = torch.quantization.quantize_dynamic(\n",
        "        model, {torch.nn.Conv2d, torch.nn.Linear}, dtype=torch.qint8\n",
        "    )\n",
        "    print(\"Quantization applied successfully!\")\n",
        "    return model_quantized\n",
        "\n",
        "def build_quantized_model(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
        "    \"\"\"\n",
        "    Builds a quantized version of the LPRNet model.\n",
        "    \"\"\"\n",
        "    model = LPRNet_quantized(lpr_max_len, phase, class_num, dropout_rate)\n",
        "\n",
        "    if phase == \"train\":\n",
        "        return model.train()\n",
        "    else:\n",
        "        return model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantized LPRNet Model Size"
      ],
      "metadata": {
        "id": "C_L8CI3t32fN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet_model = build_lprnet(lpr_max_len=8,\n",
        "        phase=False,\n",
        "        class_num=60,\n",
        "        dropout_rate=0.5)\n",
        "fused_model = fuse_lprnet_model(lprnet_model)\n",
        "quantised_model = apply_quantization(fused_model)\n",
        "quantised_model.eval()\n",
        "\n",
        "get_model_size_onnx(quantised_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6GO2yN9ThJn",
        "outputId": "0abd9b9b-8a74-4359-ecca-6f64c42d7162"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantization applied successfully!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5871267318725586"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5ifJ3TFHyZS"
      },
      "outputs": [],
      "source": [
        "def get_parser():\n",
        "    parser = argparse.ArgumentParser(description='parameters to train net')\n",
        "    parser.add_argument('--img_size', default=[94, 24], help='the image size')\n",
        "    parser.add_argument('--test_img_dirs', default=\"./data/test\", help='the test images path')\n",
        "    parser.add_argument('--dropout_rate', default=0, help='dropout rate.')\n",
        "    parser.add_argument('--lpr_max_len', default=8, help='license plate number max length.')\n",
        "    parser.add_argument('--test_batch_size', default=100, help='testing batch size.')\n",
        "    parser.add_argument('--phase_train', default=False, type=bool, help='train or test phase flag.')\n",
        "    parser.add_argument('--num_workers', default=8, type=int, help='Number of workers used in dataloading')\n",
        "    parser.add_argument('--cuda', default=True, type=bool, help='Use cuda to train model')\n",
        "    parser.add_argument('--show', default=False, type=bool, help='show test image and its predict result or not.')\n",
        "    parser.add_argument('--pretrained_model', default='./weights/Final_LPRNet_model.pth', help='pretrained base model')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    return args\n",
        "\n",
        "def collate_fn(batch):\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    lengths = []\n",
        "    for _, sample in enumerate(batch):\n",
        "        img, label, length = sample\n",
        "        imgs.append(torch.from_numpy(img))\n",
        "        labels.extend(label)\n",
        "        lengths.append(length)\n",
        "    labels = np.asarray(labels).flatten().astype(np.float32)\n",
        "\n",
        "    return (torch.stack(imgs, 0), torch.from_numpy(labels), lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTSFAGNzHHgM"
      },
      "outputs": [],
      "source": [
        "def test_quantized():\n",
        "    args = get_parser()\n",
        "\n",
        "    # Build LPRNet_quantized\n",
        "    lprnet_quantized = build_quantized_model(lpr_max_len=args.lpr_max_len, phase=args.phase_train, class_num=len(CHARS), dropout_rate=args.dropout_rate)\n",
        "    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "    lprnet_quantized.to(device)\n",
        "    print(\"Successfully built quantized network!\")\n",
        "\n",
        "    # Load the pretrained model\n",
        "    if args.pretrained_model:\n",
        "        checkpoint = torch.load(args.pretrained_model, map_location=device)\n",
        "        lprnet_quantized.load_state_dict(checkpoint, strict=False)\n",
        "        print(\"Successfully loaded quantized pretrained model!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't find pretrained model, please check!\")\n",
        "        return False\n",
        "\n",
        "    # Prepare the test dataset\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "\n",
        "    try:\n",
        "        Greedy_Decode_Eval(lprnet_quantized, test_dataset, args)\n",
        "    finally:\n",
        "        close_visualizations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iX3_Ar_kHUHU"
      },
      "outputs": [],
      "source": [
        "def Greedy_Decode_Eval(Net, datasets, args):\n",
        "    # TestNet = Net.eval()\n",
        "    epoch_size = len(datasets) // args.test_batch_size\n",
        "    batch_iterator = iter(DataLoader(datasets, args.test_batch_size, shuffle=True, num_workers=args.num_workers, collate_fn=collate_fn))\n",
        "\n",
        "    Tp = 0\n",
        "    Tn_1 = 0\n",
        "    Tn_2 = 0\n",
        "    t1 = time.time()\n",
        "    for i in range(epoch_size):\n",
        "        # load train data\n",
        "        images, labels, lengths = next(batch_iterator)\n",
        "        start = 0\n",
        "        targets = []\n",
        "        for length in lengths:\n",
        "            label = labels[start:start+length]\n",
        "            targets.append(label)\n",
        "            start += length\n",
        "        targets = np.array([el.numpy() for el in targets])\n",
        "        imgs = images.numpy().copy()\n",
        "\n",
        "        if args.cuda:\n",
        "            images = Variable(images.cuda())\n",
        "        else:\n",
        "            images = Variable(images)\n",
        "\n",
        "        # forward\n",
        "        prebs = Net(images)\n",
        "        # greedy decode\n",
        "        prebs = prebs.cpu().detach().numpy()\n",
        "        preb_labels = list()\n",
        "        for i in range(prebs.shape[0]):\n",
        "            preb = prebs[i, :, :]\n",
        "            preb_label = list()\n",
        "            for j in range(preb.shape[1]):\n",
        "                preb_label.append(np.argmax(preb[:, j], axis=0))\n",
        "            no_repeat_blank_label = list()\n",
        "            pre_c = preb_label[0]\n",
        "            if pre_c != len(CHARS) - 1:\n",
        "                no_repeat_blank_label.append(pre_c)\n",
        "            for c in preb_label: # dropout repeate label and blank label\n",
        "                if (pre_c == c) or (c == len(CHARS) - 1):\n",
        "                    if c == len(CHARS) - 1:\n",
        "                        pre_c = c\n",
        "                    continue\n",
        "                no_repeat_blank_label.append(c)\n",
        "                pre_c = c\n",
        "            preb_labels.append(no_repeat_blank_label)\n",
        "        for i, label in enumerate(preb_labels):\n",
        "            # show image and its predict label\n",
        "            if args.show:\n",
        "                show(imgs[i], label, targets[i])\n",
        "            if len(label) != len(targets[i]):\n",
        "                Tn_1 += 1\n",
        "                continue\n",
        "            if (np.asarray(targets[i]) == np.asarray(label)).all():\n",
        "                Tp += 1\n",
        "            else:\n",
        "                Tn_2 += 1\n",
        "    Acc = Tp * 1.0 / (Tp + Tn_1 + Tn_2)\n",
        "    print(\"[Info] Test Accuracy: {} [{}:{}:{}:{}]\".format(Acc, Tp, Tn_1, Tn_2, (Tp+Tn_1+Tn_2)))\n",
        "    t2 = time.time()\n",
        "    print(\"[Info] Test Speed: {}s 1/{}]\".format((t2 - t1) / len(datasets), len(datasets)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantized Model Accuracy and Inference Speed"
      ],
      "metadata": {
        "id": "xgEEPhqg4CBS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9SHy0S8IJ8u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ac9e362-de6a-4811-f006-e5591e1a4702"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully built network!\n",
            "Quantization applied successfully!\n",
            "Successfully loaded and quantized pretrained model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-e8d1a5bce0c1>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.pretrained_model, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.9 [900:60:40:1000]\n",
            "[Info] Test Speed: 0.0005907914638519287s 1/1000]\n"
          ]
        }
      ],
      "source": [
        "def test_quantized():\n",
        "    # Manually set the arguments as needed\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.img_size = [94, 24]\n",
        "            self.test_img_dirs = './data/test'\n",
        "            self.dropout_rate = 0.5\n",
        "            self.lpr_max_len = 8\n",
        "            self.test_batch_size = 100\n",
        "            self.phase_train = False\n",
        "            self.num_workers = 8\n",
        "            self.cuda = True\n",
        "            self.show = False\n",
        "            self.pretrained_model = './weights/Final_LPRNet_model.pth'\n",
        "\n",
        "    args = Args()\n",
        "    lprnet = build_lprnet(\n",
        "        lpr_max_len=args.lpr_max_len,\n",
        "        phase=args.phase_train,\n",
        "        class_num=len(CHARS),\n",
        "        dropout_rate=args.dropout_rate\n",
        "    )\n",
        "    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "    lprnet.to(device)\n",
        "    print(\"Successfully built network!\")\n",
        "\n",
        "    if args.pretrained_model:\n",
        "        checkpoint = torch.load(args.pretrained_model, map_location=device)\n",
        "        filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in lprnet.state_dict()}\n",
        "        lprnet.load_state_dict(filtered_checkpoint, strict=False)\n",
        "        fused_model = fuse_lprnet_model(lprnet)\n",
        "        quantised_model = apply_quantization(fused_model)\n",
        "        print(\"Successfully loaded and quantized pretrained model!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't find pretrained model, please check!\")\n",
        "        return False\n",
        "\n",
        "    # Prepare the test dataset\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "\n",
        "    try:\n",
        "        Greedy_Decode_Eval(quantised_model, test_dataset, args)\n",
        "    finally:\n",
        "        close_visualizations()\n",
        "\n",
        "# Call the function\n",
        "test_quantized()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**MLC OPTIMIZATIONS**"
      ],
      "metadata": {
        "id": "L2THfrorGVS7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1. PARALLELIZATION AND VECTORIZATION**"
      ],
      "metadata": {
        "id": "TzaMu-SOGsBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class LPRNet_MLC_vectorization(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet_MLC_vectorization, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1), # 0\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),  # 2\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),    # *** 4 ***\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),  # 6\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),   # 8\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 10\n",
        "            small_basic_block(ch_in=256, ch_out=256),   # *** 11 ***\n",
        "            nn.BatchNorm2d(num_features=256),   # 12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),  # 14\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),  # 16\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),  # 18\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1), # 20\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),  # *** 22 ***\n",
        "        )\n",
        "\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448+self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]: # [2, 4, 8, 11, 22]\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            print(f\"Feature map {i} size before pooling: {f.shape}\")\n",
        "            if i in [0, 1]:\n",
        "                f = nn.AvgPool2d(kernel_size=5, stride=5)(f)\n",
        "            if i in [2]:\n",
        "                f = nn.AvgPool2d(kernel_size=(4, 10), stride=(4, 2))(f)\n",
        "            f_pow = torch.pow(f, 2)\n",
        "            f_mean = torch.mean(f_pow)\n",
        "            f = torch.div(f, f_mean)\n",
        "            print(f\"Feature map {i} size after pooling: {f.shape}\")\n",
        "            global_context.append(f)\n",
        "\n",
        "        # Debug the global context tensor sizes before concatenation\n",
        "        print(f\"Global context tensor sizes: {[f.shape for f in global_context]}\")\n",
        "\n",
        "        x = torch.cat(global_context, 1)\n",
        "        x = self.container(x)\n",
        "        logits = torch.mean(x, dim=2)\n",
        "\n",
        "        return logits\n",
        "\n",
        "def build_lprnet_vectorization(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
        "    Net = LPRNet_MLC_vectorization(lpr_max_len, phase, class_num, dropout_rate)\n",
        "    if phase == \"train\":\n",
        "        return Net.train()\n",
        "    else:\n",
        "        return Net.eval()"
      ],
      "metadata": {
        "id": "be1Fx1N2GaeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parallelized and Vectorized LPRNet Model Size"
      ],
      "metadata": {
        "id": "8NrWtrwH4Qyf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet_model_vectorization = build_lprnet_vectorization(lpr_max_len=8,\n",
        "        phase=False,\n",
        "        class_num=60,\n",
        "        dropout_rate=0.5)\n",
        "lprnet_model_vectorization.eval()\n",
        "\n",
        "get_model_size_onnx(lprnet_model_vectorization)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TVhGOgKjLjzy",
        "outputId": "2fe001cc-e5d9-4aa2-c9ab-c178df685726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature map 0 size before pooling: torch.Size([1, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([1, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([1, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([1, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([1, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([1, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([1, 60, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([1, 60, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([1, 64, 4, 18]), torch.Size([1, 128, 4, 18]), torch.Size([1, 256, 4, 18]), torch.Size([1, 60, 4, 18])]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5870962142944336"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parallelized and Vectorized LPRNet Model Accuracy and Inference Speed"
      ],
      "metadata": {
        "id": "3Z8fISiK49GY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_MLC_vectorization():\n",
        "    # Manually set the arguments as needed\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.img_size = [94, 24]\n",
        "            self.test_img_dirs = './data/test'\n",
        "            self.dropout_rate = 0.5\n",
        "            self.lpr_max_len = 8\n",
        "            self.test_batch_size = 100\n",
        "            self.phase_train = False\n",
        "            self.num_workers = 8\n",
        "            self.cuda = True\n",
        "            self.show = False\n",
        "            self.pretrained_model = './weights/Final_LPRNet_model.pth'\n",
        "\n",
        "    args = Args()\n",
        "    lprnet_MLC1 = build_lprnet_vectorization(\n",
        "        lpr_max_len=args.lpr_max_len,\n",
        "        phase=args.phase_train,\n",
        "        class_num=len(CHARS),\n",
        "        dropout_rate=args.dropout_rate\n",
        "    )\n",
        "    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "    lprnet_MLC1.to(device)\n",
        "    print(\"Successfully built network!\")\n",
        "\n",
        "    if args.pretrained_model:\n",
        "        checkpoint = torch.load(args.pretrained_model, map_location=device)\n",
        "        filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in lprnet_MLC1.state_dict()}\n",
        "        lprnet_MLC1.load_state_dict(filtered_checkpoint, strict=False)\n",
        "        print(\"Successfully loaded pretrained model!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't find pretrained model, please check!\")\n",
        "        return False\n",
        "\n",
        "    # Prepare the test dataset\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "\n",
        "    try:\n",
        "        Greedy_Decode_Eval(lprnet_MLC1, test_dataset, args)\n",
        "    finally:\n",
        "        close_visualizations()\n",
        "# Call the function\n",
        "test_MLC_vectorization()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mG5pjLbGxAL",
        "outputId": "e03b36bb-75c5-46af-c702-30b5dc4379f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully built network!\n",
            "Successfully loaded pretrained model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-d5b3c57cf92f>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.pretrained_model, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "Feature map 0 size before pooling: torch.Size([100, 64, 22, 92])\n",
            "Feature map 0 size after pooling: torch.Size([100, 64, 4, 18])\n",
            "Feature map 1 size before pooling: torch.Size([100, 128, 20, 90])\n",
            "Feature map 1 size after pooling: torch.Size([100, 128, 4, 18])\n",
            "Feature map 2 size before pooling: torch.Size([100, 256, 18, 44])\n",
            "Feature map 2 size after pooling: torch.Size([100, 256, 4, 18])\n",
            "Feature map 3 size before pooling: torch.Size([100, 68, 4, 18])\n",
            "Feature map 3 size after pooling: torch.Size([100, 68, 4, 18])\n",
            "Global context tensor sizes: [torch.Size([100, 64, 4, 18]), torch.Size([100, 128, 4, 18]), torch.Size([100, 256, 4, 18]), torch.Size([100, 68, 4, 18])]\n",
            "[Info] Test Accuracy: 0.899 [899:59:42:1000]\n",
            "[Info] Test Speed: 0.0006194632053375244s 1/1000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2. LOOP BLOCKING**"
      ],
      "metadata": {
        "id": "OYQi4QIuE0-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def te_conv2d(input, weight, stride, padding):\n",
        "    N, C, H, W = input.shape\n",
        "    K, _, R, S = weight.shape\n",
        "    P = (H + 2 * padding - R) // stride + 1\n",
        "    Q = (W + 2 * padding - S) // stride + 1\n",
        "\n",
        "    rc = te.reduce_axis((0, C), name='rc')\n",
        "    ry = te.reduce_axis((0, R), name='ry')\n",
        "    rx = te.reduce_axis((0, S), name='rx')\n",
        "\n",
        "    padded_input = te.compute(\n",
        "        (N, C, H + 2 * padding, W + 2 * padding),\n",
        "        lambda n, c, h, w: te.if_then_else(\n",
        "            te.all(h >= padding, h < H + padding, w >= padding, w < W + padding),\n",
        "            input[n, c, h - padding, w - padding],\n",
        "            0.0\n",
        "        ),\n",
        "        name=\"padded_input\"\n",
        "    )\n",
        "\n",
        "    output = te.compute(\n",
        "        (N, K, P, Q),\n",
        "        lambda n, k, p, q: te.sum(\n",
        "            padded_input[n, rc, p * stride + ry, q * stride + rx] * weight[k, rc, ry, rx],\n",
        "            axis=[rc, ry, rx]\n",
        "        ),\n",
        "        name=\"conv2d\"\n",
        "    )\n",
        "    return output\n",
        "\n",
        "# Step 2: Define Tensor Expression for ReLU\n",
        "def te_relu(input):\n",
        "    return te.compute(input.shape, lambda *i: te.max(input(*i), 0), name=\"relu\")\n",
        "\n",
        "# Step 3: Define Tensor Expression for Average Pooling\n",
        "def te_avg_pool(input, pool_size, stride):\n",
        "    N, C, H, W = input.shape\n",
        "    PH, PW = pool_size\n",
        "    SH, SW = stride\n",
        "    pooled_height = (H - PH) // SH + 1\n",
        "    pooled_width = (W - PW) // SW + 1\n",
        "\n",
        "    ph = te.reduce_axis((0, PH), name=\"ph\")\n",
        "    pw = te.reduce_axis((0, PW), name=\"pw\")\n",
        "\n",
        "    return te.compute(\n",
        "        (N, C, pooled_height, pooled_width),\n",
        "        lambda n, c, h, w: te.sum(input[n, c, h * SH + ph, w * SW + pw] / (PH * PW), axis=[ph, pw]),\n",
        "        name=\"avg_pool\"\n",
        "    )\n",
        "\n",
        "# Step 4: Create TVM-LPRNet Backbone\n",
        "def build_lprnet_loopblocking(lpr_max_len, phase, class_num, dropout_rate):\n",
        "    # Input placeholders\n",
        "    N, C, H, W = 1, 3, 24, 94  # Example input size\n",
        "    input = te.placeholder((N, C, H, W), name=\"input\")\n",
        "\n",
        "    # Define layers using TVM operations\n",
        "    weight1 = te.placeholder((64, C, 3, 3), name=\"weight1\")\n",
        "    conv1 = te_conv2d(input, weight1, stride=1, padding=1)\n",
        "    relu1 = te_relu(conv1)\n",
        "\n",
        "    weight2 = te.placeholder((128, 64, 1, 1), name=\"weight2\")\n",
        "    conv2 = te_conv2d(relu1, weight2, stride=1, padding=0)\n",
        "    relu2 = te_relu(conv2)\n",
        "\n",
        "    # Add more layers as needed using `te_conv2d`, `te_relu`, etc.\n",
        "    output = relu2\n",
        "\n",
        "    # Scheduling: Apply loop blocking\n",
        "    s = te.create_schedule(output.op)\n",
        "    n, c, h, w = s[output].op.axis\n",
        "    ho, hi = s[output].split(h, factor=8)\n",
        "    wo, wi = s[output].split(w, factor=8)\n",
        "    s[output].reorder(n, c, ho, wo, hi, wi)\n",
        "\n",
        "    return s, [input, weight1, weight2, output]\n"
      ],
      "metadata": {
        "id": "zBdF5dIHR1If"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class small_basic_block(nn.Module):\n",
        "    def __init__(self, ch_in, ch_out):\n",
        "        super(small_basic_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(ch_in, ch_out // 4, kernel_size=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(3, 1), padding=(1, 0)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out // 4, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(ch_out // 4, ch_out, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class LPRNet_loop_blocking(nn.Module):\n",
        "    def __init__(self, lpr_max_len, phase, class_num, dropout_rate):\n",
        "        super(LPRNet_loop_blocking, self).__init__()\n",
        "        self.phase = phase\n",
        "        self.lpr_max_len = lpr_max_len\n",
        "        self.class_num = class_num\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.BatchNorm2d(num_features=64),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(1, 1, 1)),\n",
        "            small_basic_block(ch_in=64, ch_out=128),\n",
        "            nn.BatchNorm2d(num_features=128),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(2, 1, 2)),\n",
        "            small_basic_block(ch_in=64, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            small_basic_block(ch_in=256, ch_out=256),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool3d(kernel_size=(1, 3, 3), stride=(4, 1, 2)),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=64, out_channels=256, kernel_size=(1, 4), stride=1),\n",
        "            nn.BatchNorm2d(num_features=256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Conv2d(in_channels=256, out_channels=class_num, kernel_size=(13, 1), stride=1),\n",
        "            nn.BatchNorm2d(num_features=class_num),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.container = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=448 + self.class_num, out_channels=self.class_num, kernel_size=(1, 1), stride=(1, 1)),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        keep_features = list()\n",
        "        for i, layer in enumerate(self.backbone.children()):\n",
        "            x = layer(x)\n",
        "            if i in [2, 6, 13, 22]:  # Blocked feature selection\n",
        "                keep_features.append(x)\n",
        "\n",
        "        global_context = list()\n",
        "        for i, f in enumerate(keep_features):\n",
        "            # Apply loop blocking during pooling\n",
        "            if i in [0, 1]:\n",
        "                f = F.avg_pool2d(f, kernel_size=5, stride=5)\n",
        "            elif i == 2:\n",
        "                f = F.avg_pool2d(f, kernel_size=(4, 10), stride=(4, 2))\n",
        "\n",
        "            # Normalize each feature map\n",
        "            f_pow = f ** 2\n",
        "            f_mean = torch.mean(f_pow, dim=[1, 2, 3], keepdim=True)\n",
        "            f = f / (f_mean + 1e-8)  # Avoid division by zero\n",
        "            global_context.append(f)\n",
        "\n",
        "        # Concatenate global context tensors with loop blocking\n",
        "        x = torch.cat(global_context, dim=1)\n",
        "        x = self.container(x)\n",
        "\n",
        "        # Compute logits\n",
        "        logits = torch.mean(x, dim=2)\n",
        "        return logits\n",
        "\n",
        "def build_lprnet_loop_blocking(lpr_max_len=8, phase=False, class_num=66, dropout_rate=0.5):\n",
        "    Net = LPRNet_loop_blocking(lpr_max_len, phase, class_num, dropout_rate)\n",
        "\n",
        "    if phase == \"train\":\n",
        "        return Net.train()\n",
        "    else:\n",
        "        return Net.eval()"
      ],
      "metadata": {
        "id": "tRXsN-4o4jzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop Blocked LPRNet Model Size"
      ],
      "metadata": {
        "id": "mfbDzrgW5Hk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lprnet_loop_blocking = build_lprnet_loop_blocking(lpr_max_len=8,\n",
        "        phase=False,\n",
        "        class_num=60,\n",
        "        dropout_rate=0.5)\n",
        "lprnet_loop_blocking.eval()\n",
        "\n",
        "get_model_size_onnx(lprnet_loop_blocking)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42OyXBu65xOn",
        "outputId": "d3d8a9bf-242e-4181-957c-bb017ca1e20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.5876893997192383"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop Blocked LPRNet Model Accuracy and Inference Speed"
      ],
      "metadata": {
        "id": "DRm3STtx5Tgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loop_blocking():\n",
        "    # Manually set the arguments as needed\n",
        "    class Args:\n",
        "        def __init__(self):\n",
        "            self.img_size = [94, 24]\n",
        "            self.test_img_dirs = './data/test'\n",
        "            self.dropout_rate = 0.5\n",
        "            self.lpr_max_len = 8\n",
        "            self.test_batch_size = 100\n",
        "            self.phase_train = False\n",
        "            self.num_workers = 8\n",
        "            self.cuda = True\n",
        "            self.show = False\n",
        "            self.pretrained_model = './weights/Final_LPRNet_model.pth'\n",
        "\n",
        "    args = Args()\n",
        "    lprnet_loop_blocking = build_lprnet_loop_blocking(\n",
        "        lpr_max_len=args.lpr_max_len,\n",
        "        phase=args.phase_train,\n",
        "        class_num=len(CHARS),\n",
        "        dropout_rate=args.dropout_rate\n",
        "    )\n",
        "    device = torch.device(\"cuda:0\" if args.cuda else \"cpu\")\n",
        "    lprnet_loop_blocking.to(device)\n",
        "    print(\"Successfully built network!\")\n",
        "\n",
        "    if args.pretrained_model:\n",
        "        checkpoint = torch.load(args.pretrained_model, map_location=device)\n",
        "        filtered_checkpoint = {k: v for k, v in checkpoint.items() if k in lprnet_loop_blocking.state_dict()}\n",
        "        lprnet_loop_blocking.load_state_dict(filtered_checkpoint, strict=False)\n",
        "        print(\"Successfully loaded pretrained model!\")\n",
        "    else:\n",
        "        print(\"[Error] Can't find pretrained model, please check!\")\n",
        "        return False\n",
        "\n",
        "    # Prepare the test dataset\n",
        "    test_img_dirs = os.path.expanduser(args.test_img_dirs)\n",
        "    test_dataset = LPRDataLoader(test_img_dirs.split(','), args.img_size, args.lpr_max_len)\n",
        "\n",
        "    try:\n",
        "        Greedy_Decode_Eval(lprnet_loop_blocking, test_dataset, args)\n",
        "    finally:\n",
        "        close_visualizations()\n",
        "\n",
        "# Call the function\n",
        "test_loop_blocking()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o38Qyl57E6C",
        "outputId": "4394ebee-abe6-4173-c829-c96442ae6db0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully built network!\n",
            "Successfully loaded pretrained model!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-47-74581baa1077>:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.pretrained_model, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Info] Test Accuracy: 0.894 [894:69:37:1000]\n",
            "[Info] Test Speed: 0.000720259666442871s 1/1000]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}